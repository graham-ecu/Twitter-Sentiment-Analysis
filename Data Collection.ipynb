{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90f9367f",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "\n",
    "In this notebook tweets containing the keyword \"Tesla\" will be collected using Twitter's API.  Two .csv files will be saved at the end of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f213a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleted before uploading notebook\n",
    "api_key = ...\n",
    "api_key_secret = ...\n",
    "bearer_token = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dae8eb",
   "metadata": {},
   "source": [
    "The code to collect tweets was based on a combination of the Twitter API documentation and the instructions in the link below.\n",
    "\n",
    "https://towardsdatascience.com/an-extensive-guide-to-collecting-tweets-from-twitter-api-v2-for-academic-research-using-python-3-518fcb71df2a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e913a844",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d619d5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For sending GET requests from the API\n",
    "import requests\n",
    "# For file management when creating and adding to the dataset\n",
    "import os\n",
    "# For dealing with json responses from the API\n",
    "import json\n",
    "# For displaying the data after\n",
    "import pandas as pd\n",
    "# For saving the response data in CSV format\n",
    "import csv\n",
    "# For parsing the dates received from twitter in readable formats\n",
    "import datetime\n",
    "import dateutil.parser\n",
    "import unicodedata\n",
    "#To add wait time between requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f606dc23",
   "metadata": {},
   "source": [
    "## Defining useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8095c286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates headers based on bearer token\n",
    "def create_headers(bearer_token):\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    return headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3463a7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates url with easy to modify inputs\n",
    "def create_url(keyword, start_date, end_date, max_results = 100):\n",
    "    \n",
    "    search_url = \"https://api.twitter.com/2/tweets/search/recent\"\n",
    "\n",
    "    # set params\n",
    "    query_params = {'query': keyword,\n",
    "                    'start_time': start_date,\n",
    "                    'end_time': end_date,\n",
    "                    'max_results': max_results,\n",
    "                    'tweet.fields': 'created_at',\n",
    "                    'next_token': {}}\n",
    "    return (search_url, query_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b034e181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# makes request and prints response code so I know if it worked or not\n",
    "def connect_to_endpoint(url, headers, params, next_token=None):\n",
    "    \n",
    "    params['next_token'] = next_token\n",
    "    \n",
    "    response = requests.request(\"GET\", url, headers = headers, params = params)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "        \n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fb8c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_csv(json_response, fileName):\n",
    "\n",
    "    #Open OR create the target CSV file\n",
    "    csvFile = open(fileName, \"a\", newline=\"\", encoding='utf-8')\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "\n",
    "    #Loop through each tweet\n",
    "    for tweet in json_response['data']:\n",
    "\n",
    "        tweet_id = tweet['id']\n",
    "\n",
    "        text = tweet['text']\n",
    "        \n",
    "        date = tweet['created_at']\n",
    "        \n",
    "        # Assemble data in a list\n",
    "        res = [tweet_id, text ,date]\n",
    "        \n",
    "        # Append the result to the CSV file\n",
    "        csvWriter.writerow(res)\n",
    "\n",
    "    # When done, close the CSV file\n",
    "    csvFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b1ef18",
   "metadata": {},
   "source": [
    "## Collecting data before Q4 earnings announcement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d034857d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inputs for the request\n",
    "headers = create_headers(bearer_token)\n",
    "keyword = \"Tesla OR tesla OR TSLA OR tsla lang:en\"\n",
    "start_time = \"2022-01-21T00:00:00.000Z\"\n",
    "end_time = \"2022-01-22T00:00:00.000Z\" # 24 hours after start\n",
    "max_results = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8ac0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = create_url(keyword, start_time,end_time, max_results)\n",
    "json_response = connect_to_endpoint(url[0], headers, url[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9cdc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(json_response, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461d3a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create file\n",
    "csvFile = open(\"before.csv\", \"a\", newline=\"\", encoding='utf-8')\n",
    "csvWriter = csv.writer(csvFile)\n",
    "\n",
    "# Create column names\n",
    "csvWriter.writerow(['id', 'text', 'created_at'])\n",
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ee6865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append first response\n",
    "append_to_csv(json_response, \"before.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c33b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append tweets on the next 399 pages\n",
    "# There is a rate limit of 400 requests in 15 minutes, so the loop will stop at 400 requests\n",
    "for i in range(400):\n",
    "    if 'next_token' in json_response['meta']:\n",
    "        next_token = json_response['meta']['next_token']\n",
    "    \n",
    "        url = create_url(keyword, start_time, end_time, max_results)\n",
    "        json_response = connect_to_endpoint(url[0], headers, url[1], next_token);\n",
    "    \n",
    "        append_to_csv(json_response, \"before.csv\")\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "print('All tweets collected')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bdf9b1",
   "metadata": {},
   "source": [
    "## Collecting Data After the Q4 earnings announcement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afcc247",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inputs for the request\n",
    "headers = create_headers(bearer_token)\n",
    "keyword = \"Tesla lang:en\"\n",
    "start_time = \"2022-01-26T18:00:00.000Z\" # Note that the time is set for 6:00 PM\n",
    "end_time = \"2022-01-27T18:00:00.000Z\" # 24 hours after start\n",
    "max_results = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31014fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = create_url(keyword, start_time,end_time, max_results)\n",
    "json_response = connect_to_endpoint(url[0], headers, url[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d82b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(json_response, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f54f7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create file\n",
    "csvFile = open(\"after.csv\", \"a\", newline=\"\", encoding='utf-8')\n",
    "csvWriter = csv.writer(csvFile)\n",
    "\n",
    "#Create column names\n",
    "csvWriter.writerow(['id', 'text', 'created_at'])\n",
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1995dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append first response\n",
    "append_to_csv(json_response, \"after.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7808d376",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(400):\n",
    "    if 'next_token' in json_response['meta']:\n",
    "        next_token = json_response['meta']['next_token']\n",
    "    \n",
    "        url = create_url(keyword, start_time, end_time, max_results)\n",
    "        json_response = connect_to_endpoint(url[0], headers, url[1], next_token)\n",
    "    \n",
    "        append_to_csv(json_response, \"after.csv\")\n",
    "    else:\n",
    "        print('All tweets collected')   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
